{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasleema17/FINAL_PROJECT_22034202/blob/main/FloodPredictionCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Import Libraries and Packages**\n",
        "\n",
        "Load the essential libraries needed for data processing, visualization, and model building."
      ],
      "metadata": {
        "id": "p24N2GK8ZRtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "J8ORYWjNYCar"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Load and Preprocess Dataset**\n",
        "\n",
        "Import the dataset, clean it by handling missing values, outliers, and inconsistencies to ensure it’s ready for analysis"
      ],
      "metadata": {
        "id": "qW9VuPTRc8n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data flies\n",
        "flood_data = pd.read_csv(r\"/content/IndiaFloodInventory.csv\")\n",
        "rainfall_data = pd.read_csv(r'/content/Sub_Division_IMD_2017.csv')"
      ],
      "metadata": {
        "id": "RIXu9BCRYCd5"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of each dataset\n",
        "print(flood_data.head())\n",
        "print(rainfall_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvq5pH-aYCg9",
        "outputId": "fbfc9a71-496c-4639-9f7e-948cac17cc36"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                   UEI        Start Date          End Date  \\\n",
            "0         563  UEI-IMD-FL-1967-0001  02/07/1967 00:00  08/07/1967 00:00   \n",
            "1         564  UEI-IMD-FL-1967-0002  22/07/1967 00:00  28/07/1967 00:00   \n",
            "2         565  UEI-IMD-FL-1967-0003  01/08/1967 00:00  30/08/1967 00:00   \n",
            "3         566  UEI-IMD-FL-1967-0004  08/09/1967 00:00  09/09/1967 00:00   \n",
            "4         567  UEI-IMD-FL-1968-0001  22/06/1968 00:00  28/06/1968 00:00   \n",
            "\n",
            "   Duration(Days) Main Cause  \\\n",
            "0             7.0      flood   \n",
            "1             7.0      flood   \n",
            "2            30.0      flood   \n",
            "3             2.0      flood   \n",
            "4             7.0      flood   \n",
            "\n",
            "                                           Districts  \\\n",
            "0                                                NaN   \n",
            "1                                                NaN   \n",
            "2                                                NaN   \n",
            "3  Bhadrak, Dhenkanal, Jajapur, Subarnapur, Nuapa...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                               State  \\\n",
            "0                                              Assam   \n",
            "1                                Maharastra, Gujarat   \n",
            "2                                      Uttar Pradesh   \n",
            "3  Odisha, Bihar, Uttar Pradesh, Madhya Pradesh, ...   \n",
            "4                                              Assam   \n",
            "\n",
            "                                   Extent of damage  Event Source  \n",
            "0                                                NaN          IMD  \n",
            "1                                                NaN          IMD  \n",
            "2                                                NaN          IMD  \n",
            "3                        Marooning of Barwer Express          IMD  \n",
            "4  Affecting about one million people,an area of ...          IMD  \n",
            "                 SUBDIVISION  YEAR   JAN    FEB   MAR    APR    MAY    JUN  \\\n",
            "0  Andaman & Nicobar Islands  1901  49.2   87.1  29.2    2.3  528.8  517.5   \n",
            "1  Andaman & Nicobar Islands  1902   0.0  159.8  12.2    0.0  446.1  537.1   \n",
            "2  Andaman & Nicobar Islands  1903  12.7  144.0   0.0    1.0  235.1  479.9   \n",
            "3  Andaman & Nicobar Islands  1904   9.4   14.7   0.0  202.4  304.5  495.1   \n",
            "4  Andaman & Nicobar Islands  1905   1.3    0.0   3.3   26.9  279.5  628.7   \n",
            "\n",
            "     JUL    AUG    SEP    OCT    NOV    DEC  ANNUAL     JF    MAM    JJAS  \\\n",
            "0  365.1  481.1  332.6  388.5  558.2   33.6  3373.2  136.3  560.3  1696.3   \n",
            "1  228.9  753.7  666.2  197.2  359.0  160.5  3520.7  159.8  458.3  2185.9   \n",
            "2  728.4  326.7  339.0  181.2  284.4  225.0  2957.4  156.7  236.1  1874.0   \n",
            "3  502.0  160.1  820.4  222.2  308.7   40.1  3079.6   24.1  506.9  1977.6   \n",
            "4  368.7  330.5  297.0  260.7   25.4  344.7  2566.7    1.3  309.7  1624.9   \n",
            "\n",
            "     OND  \n",
            "0  980.3  \n",
            "1  716.7  \n",
            "2  690.6  \n",
            "3  571.0  \n",
            "4  630.8  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FLOOD DATA\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "flood_data['Start Date'] = pd.to_datetime(flood_data['Start Date'], format='%d/%m/%Y %H:%M')\n",
        "flood_data['End Date'] = pd.to_datetime(flood_data['End Date'], format='%d/%m/%Y %H:%M')\n",
        "# Extract date features\n",
        "flood_data['Year'] = flood_data['Start Date'].dt.year\n",
        "flood_data['Month'] = flood_data['Start Date'].dt.month\n",
        "flood_data['Day'] = flood_data['Start Date'].dt.day\n",
        "flood_data['Season'] = flood_data['Start Date'].dt.month % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Monsoon\n",
        "flood_data['Day of Week'] = flood_data['Start Date'].dt.dayofweek\n",
        "\n",
        "#RAINFALL DATA\n",
        "\n",
        "# Melt rainfall_data to get a 'MONTH' and 'RAIN' column\n",
        "rainfall_data_melted = rainfall_data.melt(\n",
        "    id_vars=['SUBDIVISION', 'YEAR'],\n",
        "    value_vars=['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC'],\n",
        "    var_name='MONTH',\n",
        "    value_name='RAIN'\n",
        ")\n",
        "\n",
        "# Convert month abbreviations to numeric values (1 for January, 2 for February, etc.)\n",
        "month_map = {\n",
        "    'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,\n",
        "    'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12\n",
        "}\n",
        "rainfall_data_melted['MONTH'] = rainfall_data_melted['MONTH'].map(month_map)\n",
        "\n",
        "# Ensure 'YEAR' is of type int\n",
        "rainfall_data_melted['YEAR'] = rainfall_data_melted['YEAR'].astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "o2XDvXrQYCj2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MERGE BOTH DATASETS\n",
        "# Now merge with flood_data on 'Year' and 'Month'\n",
        "merged_data = flood_data.merge(\n",
        "    rainfall_data_melted,\n",
        "    left_on=['Year', 'Month'],\n",
        "    right_on=['YEAR', 'MONTH'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing rainfall data with 0\n",
        "merged_data['RAIN'] = merged_data['RAIN'].fillna(0)\n",
        "# Drop unnecessary columns after merging\n",
        "merged_data = merged_data.drop(columns=['YEAR', 'UEI', 'Event Source', 'Year', 'Month', 'Day', 'Districts', 'Extent of damage ','Unnamed: 0'])"
      ],
      "metadata": {
        "id": "Cks2IdYeYCmv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'State' column is of type string\n",
        "merged_data['State'] = merged_data['State'].astype(str)\n",
        "\n",
        "# Ensure that NaN values in 'State' are replaced with an empty string, then convert to string\n",
        "merged_data['State'] = merged_data['State'].replace(pd.NA, '').astype(str)\n",
        "\n",
        "# Split the 'State' column by commas and explode into separate rows\n",
        "merged_data['State'] = merged_data['State'].str.split(', ')\n",
        "merged_data_exploded = merged_data.explode('State')\n",
        "\n",
        "# Remove any rows where 'State' is empty (after handling NaN values)\n",
        "merged_data_exploded = merged_data_exploded[merged_data_exploded['State'] != '']\n",
        "\n",
        "# Clean the 'State' column to remove extraneous characters\n",
        "merged_data_exploded['State'] = merged_data_exploded['State'].str.replace(\n",
        "    r\"[^a-zA-Z,& ]\", \"\", regex=True\n",
        ")\n",
        "# Check the shape of the DataFrame\n",
        "rows, columns = merged_data_exploded.shape\n",
        "print(f\"The DataFrame has {rows} rows and {columns} columns.\")\n"
      ],
      "metadata": {
        "id": "N75wPg02pTtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe5f70e-175c-49f9-c779-22427f191531"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DataFrame has 161268 rows and 10 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows\n",
        "merged_data_exploded = merged_data_exploded.drop_duplicates()\n",
        "\n",
        "# Handle missing values using .loc\n",
        "# For 'Main Cause', fill with 'Unknown'\n",
        "merged_data_exploded.loc[:, 'Main Cause'] = merged_data_exploded['Main Cause'].fillna('Unknown')\n",
        "\n",
        "# For 'SUBDIVISION', fill with 'Unknown'\n",
        "merged_data_exploded.loc[:, 'SUBDIVISION'] = merged_data_exploded['SUBDIVISION'].fillna('Unknown')\n",
        "\n",
        "# For 'RAIN', fill with the median value\n",
        "median_rain = merged_data_exploded['RAIN'].median()\n",
        "merged_data_exploded.loc[:, 'RAIN'] = merged_data_exploded['RAIN'].fillna(median_rain)\n",
        "\n",
        "# Assuming 'merged_data_exploded' is your DataFrame\n",
        "merged_data_exploded = merged_data_exploded.dropna(subset=['MONTH'])\n",
        "\n",
        "# Verify the data cleaning\n",
        "print(\"Missing values after cleaning:\\n\", merged_data_exploded.isnull().sum())\n",
        "print(f\"\\nNumber of rows and columns after cleaning: {merged_data_exploded.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRjxP4sNk_kE",
        "outputId": "e2cb9ee3-0e0c-4bd8-aa64-f7b056371123"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after cleaning:\n",
            " Start Date        0\n",
            "End Date          0\n",
            "Duration(Days)    0\n",
            "Main Cause        0\n",
            "State             0\n",
            "Season            0\n",
            "Day of Week       0\n",
            "SUBDIVISION       0\n",
            "MONTH             0\n",
            "RAIN              0\n",
            "dtype: int64\n",
            "\n",
            "Number of rows and columns after cleaning: (157248, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'floods' and 'heavy floods' with 'flood' in \"Main Cause\"\n",
        "merged_data_exploded['Main Cause'] = merged_data_exploded['Main Cause'].str.lower().replace(\n",
        "    {'floods': 'flood', 'heavy floods': 'flood'}, regex=True\n",
        ")\n",
        "\n",
        "# Assuming your dataframe is named 'merged_data_exploded'\n",
        "def categorize_cause(cause):\n",
        "    # Convert to lowercase for uniform comparison\n",
        "    cause_lower = cause.lower()\n",
        "\n",
        "    # Category 1: Any term implying \"flood\" or \"flash flood\"\n",
        "    if \"flood\" in cause_lower:\n",
        "        return \"flood\"\n",
        "    # Category 2: Descriptions mentioning \"heavy rains,\" \"landslide,\" or \"cloud burst\"\n",
        "    elif any(term in cause_lower for term in [\"heavy rains\", \"landslide\", \"cloud burst\", \"heavy rain\"]):\n",
        "        return \"no flood\"\n",
        "    # Default to \"no flood\" if it doesn't match any criteria\n",
        "    else:\n",
        "        return \"no flood\"\n",
        "\n",
        "# Apply the categorization function to the 'Main Cause' column\n",
        "merged_data_exploded['Flood Occurance'] = merged_data_exploded['Main Cause'].apply(categorize_cause)\n",
        "# Replace 'flood' with 1 and 'no flood' with 0 in the 'Flood Occurrence' column\n",
        "merged_data_exploded['Flood Occurance'] = merged_data_exploded['Flood Occurance'].replace({'flood': 1, 'no flood': 0})\n",
        "\n",
        "merged_data_exploded = merged_data_exploded.drop(columns=['Main Cause'])\n",
        "\n",
        "# Count the occurrences of 'flood' and 'no flood' in the 'Flood Occurance' column\n",
        "flood_counts = merged_data_exploded['Flood Occurance'].value_counts()\n",
        "\n",
        "# Display the result\n",
        "print(flood_counts)"
      ],
      "metadata": {
        "id": "Ud09jmiqm_ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dcb2570-cb13-43c2-f4e1-e7039f8321b3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flood Occurance\n",
            "0    121896\n",
            "1     35352\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-dce49864dc12>:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  merged_data_exploded['Flood Occurance'] = merged_data_exploded['Flood Occurance'].replace({'flood': 1, 'no flood': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define corrections dictionary to standardize state names\n",
        "state_corrections = {\n",
        "    \"Jammu & Kashmir\": \"Jammu And Kashmir\",\n",
        "    \"Daman & Diu\": \"Daman And Diu\",\n",
        "    \"Maharastra\": \"Maharashtra\",\n",
        "    \"Madras\": \"Tamil Nadu\",\n",
        "    \"Tamilnadu\": \"Tamil Nadu\",\n",
        "    \"Chennai\": \"Tamil Nadu\",\n",
        "    \"Daman And Diu\": \"Daman,Diu\",\n",
        "    \"Parts Of Maharastra\": \"Maharashtra\",\n",
        "    \"East Rajasthan\": \"Rajasthan\"\n",
        "\n",
        "}\n",
        "# Clean the 'State' column: remove leading/trailing spaces, apply title case, and apply corrections\n",
        "merged_data_exploded['State'] = (\n",
        "    merged_data_exploded['State']\n",
        "    .str.strip()\n",
        "    .str.title()\n",
        "    .replace(state_corrections)\n",
        ")"
      ],
      "metadata": {
        "id": "XBOawEjPhFRe"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_exploded.head(157248))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpkAq_tIhOuM",
        "outputId": "fce14662-a507-44e9-8841-563908138118"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Start Date   End Date  Duration(Days)        State  Season  \\\n",
            "0      1967-07-02 1967-07-08             7.0        Assam     3.0   \n",
            "1      1967-07-02 1967-07-08             7.0        Assam     3.0   \n",
            "2      1967-07-02 1967-07-08             7.0        Assam     3.0   \n",
            "3      1967-07-02 1967-07-08             7.0        Assam     3.0   \n",
            "4      1967-07-02 1967-07-08             7.0        Assam     3.0   \n",
            "...           ...        ...             ...          ...     ...   \n",
            "154653 2017-08-08 2017-08-14             7.0  West Bengal     3.0   \n",
            "154654 2017-08-08 2017-08-14             7.0  West Bengal     3.0   \n",
            "154655 2017-08-08 2017-08-14             7.0  West Bengal     3.0   \n",
            "154656 2017-08-08 2017-08-14             7.0  West Bengal     3.0   \n",
            "154657 2017-08-08 2017-08-14             7.0  West Bengal     3.0   \n",
            "\n",
            "        Day of Week                         SUBDIVISION  MONTH   RAIN  \\\n",
            "0               6.0           Andaman & Nicobar Islands    7.0  537.8   \n",
            "1               6.0                   Arunachal Pradesh    7.0  434.5   \n",
            "2               6.0                   Assam & Meghalaya    7.0  497.7   \n",
            "3               6.0              Naga Mani Mizo Tripura    7.0  388.0   \n",
            "4               6.0  Sub Himalayan West Bengal & Sikkim    7.0  901.6   \n",
            "...             ...                                 ...    ...    ...   \n",
            "154653          1.0                   Coastal Karnataka    8.0  625.2   \n",
            "154654          1.0            North Interior Karnataka    8.0  105.8   \n",
            "154655          1.0            South Interior Karnataka    8.0  181.4   \n",
            "154656          1.0                              Kerala    8.0  462.6   \n",
            "154657          1.0                         Lakshadweep    8.0  206.2   \n",
            "\n",
            "        Flood Occurance  \n",
            "0                     1  \n",
            "1                     1  \n",
            "2                     1  \n",
            "3                     1  \n",
            "4                     1  \n",
            "...                 ...  \n",
            "154653                1  \n",
            "154654                1  \n",
            "154655                1  \n",
            "154656                1  \n",
            "154657                1  \n",
            "\n",
            "[157248 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_exploded.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyHeas-Se2yZ",
        "outputId": "c03148a1-e7ea-4c02-e542-21ac4d034a49"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Date         0\n",
            "End Date           0\n",
            "Duration(Days)     0\n",
            "State              0\n",
            "Season             0\n",
            "Day of Week        0\n",
            "SUBDIVISION        0\n",
            "MONTH              0\n",
            "RAIN               0\n",
            "Flood Occurance    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Separate features (X) and target (y)\n",
        "X = merged_data_exploded.drop(columns=['Flood Occurance'])\n",
        "y = merged_data_exploded['Flood Occurance']\n",
        "\n",
        "# Step 2: Apply Random UnderSampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "# Step 3: Combine X_resampled and y_resampled to create the resampled dataset\n",
        "merged_data_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# Step 4: Verify the balanced distribution of 'Flood Occurance'\n",
        "print(\"Balanced Class Distribution (After Undersampling):\\n\", merged_data_resampled['Flood Occurance'].value_counts())\n",
        "\n",
        "# Optionally, update merged_data_exploded with the resampled data\n",
        "merged_data_exploded = merged_data_resampled\n"
      ],
      "metadata": {
        "id": "2BWz1BcPUXQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d591c7-d222-4d34-ace3-0490f526cb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Class Distribution (After Undersampling):\n",
            " Flood Occurance\n",
            "0    35352\n",
            "1    35352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of 'flood' and 'no flood' in the 'Flood Occurance' column\n",
        "flood_counts = merged_data_exploded['Flood Occurance'].value_counts()\n",
        "\n",
        "# Display the result\n",
        "print(flood_counts)"
      ],
      "metadata": {
        "id": "b71PYLKeUXTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the DataFrame\n",
        "rows, columns = merged_data_exploded.shape\n",
        "print(f\"The DataFrame has {rows} rows and {columns} columns.\")\n"
      ],
      "metadata": {
        "id": "0DVzgznNUXWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSIGHTS"
      ],
      "metadata": {
        "id": "9gDrljJXUvmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Rainfall Comparison by Subdivision\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=merged_data_exploded, x='SUBDIVISION', y='RAIN', palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Rainfall by Subdivision\")\n",
        "plt.ylabel(\"Rainfall (mm)\")\n",
        "plt.xlabel(\"Subdivision\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCZ-et3hEE9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Clean the 'State' column: remove leading/trailing spaces and make all entries lowercase\n",
        "merged_data_exploded['State'] = merged_data_exploded['State'].str.strip().str.title()\n",
        "\n",
        "# Group by 'State' and calculate the average rainfall per state\n",
        "rainfall_per_state = merged_data_exploded.groupby('State')['RAIN'].mean().reset_index()\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(12, 6))  # Increase the figure size to make more room\n",
        "sns.barplot(data=rainfall_per_state, x='State', y='RAIN', palette='muted', ci=None)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Rainfall Summary per State\")\n",
        "plt.xlabel(\"State\")\n",
        "plt.ylabel(\"Average Rainfall (mm)\")\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout to ensure the labels fit nicely\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ljcT8p-QeZQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Top 10 Rainiest States During Floods\n",
        "top_states = merged_data_exploded.groupby('State')['RAIN'].mean().nlargest(10).reset_index()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=top_states, x='State', y='RAIN', palette='magma')\n",
        "plt.title(\"Top 10 Rainiest States During Floods\")\n",
        "plt.xlabel(\"State\")\n",
        "plt.ylabel(\"Average Rainfall (mm)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nLZgwmllWHyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL EVALUATION\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zKqakFPQr3bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_exploded.head())\n"
      ],
      "metadata": {
        "id": "55D-WeMlr193"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random Forest\n",
        "\n",
        "# Select features and target variable\n",
        "X = merged_data_exploded.drop(columns=['Flood Occurance', 'Start Date', 'End Date'])  # Drop unnecessary columns\n",
        "y = merged_data_exploded['Flood Occurance']\n",
        "\n",
        "#Identify numeric columns\n",
        "numeric_cols = merged_data_exploded.select_dtypes(include='number').columns\n",
        "\n",
        "#Fill missing values in numeric columns with the mean of each column\n",
        "merged_data_exploded[numeric_cols] = merged_data_exploded[numeric_cols].fillna(merged_data_exploded[numeric_cols].mean())\n",
        "\n",
        "# Alternatively, fill missing values for categorical columns if necessary\n",
        "merged_data_exploded = merged_data_exploded.fillna({'State': 'Unknown', 'SUBDIVISION': 'Unknown'})\n",
        "\n",
        "# preprocessing steps\n",
        "# Convert categorical variables to dummy/indicator variables\n",
        "#merged_data_exploded = pd.get_dummies(merged_data_exploded, columns=['State', 'SUBDIVISION'], drop_first=True)\n",
        "\n",
        "\n",
        "# Scale numerical features if necessary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X[['RAIN', 'MONTH', 'Duration(Days)']] = scaler.fit_transform(X[['RAIN', 'MONTH', 'Duration(Days)']])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#\n",
        "#print('X_train:', x_train.shape)\n",
        "#add others\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Predict probabilities for the positive class (1)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "# Calculate ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"roc-auc:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "aliTtioxxC52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get the predicted probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Dashed diagonal for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L5XSntV4tUHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features only on the training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[['RAIN', 'MONTH', 'Duration(Days)']] = scaler.fit_transform(X_train[['RAIN', 'MONTH', 'Duration(Days)']])\n",
        "\n",
        "# Apply the same scaling to the test data\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[['RAIN', 'MONTH', 'Duration(Days)']] = scaler.transform(X_test[['RAIN', 'MONTH', 'Duration(Days)']])\n",
        "\n",
        "# Initialize the Logistic Regression model with changes\n",
        "log_reg = LogisticRegression(random_state=42,\n",
        "                             max_iter=1000,  # Increased max_iter for more iterations\n",
        "                             solver='liblinear',  # Use the 'liblinear' solver which works well for smaller datasets\n",
        "                             C=1.0)  # Regularization strength (try adjusting if necessary)\n",
        "\n",
        "# Train the model with scaled training data\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate on the test set\n",
        "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
        "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
        "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Logistic Regression Model Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_log_reg)\n",
        "print(\"Precision:\", precision_log_reg)\n",
        "print(\"Recall:\", recall_log_reg)\n",
        "print(\"F1 Score:\", f1_log_reg)\n"
      ],
      "metadata": {
        "id": "UzBtMZdZb-DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM MODEL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Step 1: Select features and target variable\n",
        "X = merged_data_exploded.drop(columns=['Start Date', 'End Date', 'Flood Occurance'])  # Drop the irrelevant columns\n",
        "y = merged_data_exploded['Flood Occurance']  # Target variable\n",
        "\n",
        "# Step 2: Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Convert target variable to categorical (one-hot encoding)\n",
        "y_categorical = to_categorical(y)  # Convert the binary target to one-hot encoding\n",
        "\n",
        "# Step 4: Reshape data for LSTM (LSTM expects 3D input: [samples, timesteps, features])\n",
        "X_lstm = np.expand_dims(X_scaled, axis=1)  # Add a timestep dimension (1 timestep per sample)\n",
        "\n",
        "# Step 5: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),  # LSTM layer\n",
        "    Dropout(0.3),  # Dropout layer for regularization\n",
        "    Dense(64, activation='relu'),  # Dense hidden layer\n",
        "    Dropout(0.3),  # Dropout layer for regularization\n",
        "    Dense(y_categorical.shape[1], activation='softmax')  # Output layer with softmax for classification\n",
        "])\n",
        "\n",
        "# Step 7: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20,  # Number of epochs can be adjusted\n",
        "    batch_size=32,  # Batch size can be adjusted\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 9: Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Step 10: Make predictions (optional)\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to class labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded y_test back to labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph4hLn7x_v1_",
        "outputId": "49a18b57-c5cb-461b-e574-27b88d943a39"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.6711 - loss: 0.6050 - val_accuracy: 0.7133 - val_loss: 0.5676\n",
            "Epoch 2/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7143 - loss: 0.5651 - val_accuracy: 0.7187 - val_loss: 0.5529\n",
            "Epoch 3/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7229 - loss: 0.5523 - val_accuracy: 0.7231 - val_loss: 0.5467\n",
            "Epoch 4/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7303 - loss: 0.5411 - val_accuracy: 0.7291 - val_loss: 0.5372\n",
            "Epoch 5/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7320 - loss: 0.5325 - val_accuracy: 0.7313 - val_loss: 0.5307\n",
            "Epoch 6/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7366 - loss: 0.5226 - val_accuracy: 0.7362 - val_loss: 0.5264\n",
            "Epoch 7/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7433 - loss: 0.5108 - val_accuracy: 0.7388 - val_loss: 0.5200\n",
            "Epoch 8/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7443 - loss: 0.5104 - val_accuracy: 0.7468 - val_loss: 0.5134\n",
            "Epoch 9/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7478 - loss: 0.5061 - val_accuracy: 0.7498 - val_loss: 0.5065\n",
            "Epoch 10/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7521 - loss: 0.4955 - val_accuracy: 0.7488 - val_loss: 0.5016\n",
            "Epoch 11/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7545 - loss: 0.4914 - val_accuracy: 0.7538 - val_loss: 0.4997\n",
            "Epoch 12/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7571 - loss: 0.4887 - val_accuracy: 0.7579 - val_loss: 0.4919\n",
            "Epoch 13/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7645 - loss: 0.4785 - val_accuracy: 0.7586 - val_loss: 0.4907\n",
            "Epoch 14/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7619 - loss: 0.4753 - val_accuracy: 0.7606 - val_loss: 0.4847\n",
            "Epoch 15/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7677 - loss: 0.4696 - val_accuracy: 0.7613 - val_loss: 0.4828\n",
            "Epoch 16/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7678 - loss: 0.4672 - val_accuracy: 0.7655 - val_loss: 0.4773\n",
            "Epoch 17/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7690 - loss: 0.4610 - val_accuracy: 0.7668 - val_loss: 0.4761\n",
            "Epoch 18/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7768 - loss: 0.4538 - val_accuracy: 0.7715 - val_loss: 0.4692\n",
            "Epoch 19/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7751 - loss: 0.4520 - val_accuracy: 0.7729 - val_loss: 0.4658\n",
            "Epoch 20/20\n",
            "\u001b[1m1768/1768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7757 - loss: 0.4481 - val_accuracy: 0.7742 - val_loss: 0.4650\n",
            "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7780 - loss: 0.4641\n",
            "Test Loss: 0.46499866247177124\n",
            "Test Accuracy: 0.7742026448249817\n",
            "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)  # Predictions\n",
        "y_true = np.argmax(y_test, axis=1)  # True labels (if one-hot encoded)\n",
        "\n",
        "print(classification_report(y_true, y_pred))  # Precision, Recall, F1 Score\n",
        "roc_auc = roc_auc_score(y_true, model.predict(X_test)[:, 1])  # ROC-AUC score\n",
        "print(\"ROC-AUC score:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "t1qcNzPq_v6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# If y_test is one-hot encoded, convert it to a single label (0 or 1)\n",
        "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
        "    y_test = np.argmax(y_test, axis=1)  # Convert one-hot to single label\n",
        "\n",
        "# Predict probabilities for the positive class (Flood = 1)\n",
        "y_pred_proba = model.predict(X_test)[:, 1]\n",
        "\n",
        "# Compute the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ileW13kS_wDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert predicted probabilities to binary class labels\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "# Use y_test directly (already binary)\n",
        "y_test_labels = y_test.astype(int)  # Ensure it's integer for comparison\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"LSTM Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "oNw8BwQ8_wTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vHIe5hm_wWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VVh2iw5_wZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVr1wnRNXg0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}